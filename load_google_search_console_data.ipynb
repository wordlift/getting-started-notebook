{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Google Search Console data\n",
    "\n",
    "## Configuration\n",
    "\n",
    "There are two configuration sources, at least one of the two is needed, and they're applied in order:\n",
    "\n",
    "1. A file config/default.py\n",
    "2. Local constants and WordLift Key in Google Colab Secrets\n",
    "\n",
    "There's only one configuration settings:\n",
    "\n",
    "* `WORDLIFT_KEY`, holding the WordLift Key, when using Google Colab, it can be set in the secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "from wordlift_client import (\n",
    "    AnalyticsImportRequest,\n",
    "    AccountApi,\n",
    "    EntityGapsApi,\n",
    "    EntityGapRequest,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Configuration from config/default.py file.\n",
    "try:\n",
    "    # Configuration is in the `config/default.py` file.\n",
    "    from config import default as config\n",
    "\n",
    "    WORDLIFT_KEY = config.WORDLIFT_KEY\n",
    "    OUTPUT_TYPE = config.OUTPUT_TYPE or \"http://schema.org/WebPage\"\n",
    "    URLS = config.URLS\n",
    "except ImportError:\n",
    "    logging.warning(\"Cannot import configuration from local `config/default.py` file.\")\n",
    "\n",
    "# Configuration from Google Colab Secrets.\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    WORDLIFT_KEY = userdata.get(\"WORDLIFT_KEY\")\n",
    "    OUTPUT_TYPE = \"http://schema.org/WebPage\"\n",
    "    URLS = []\n",
    "except ImportError:\n",
    "    logging.warning(\"Cannot import configuration from google.colab.usermap.\")\n",
    "\n",
    "if WORDLIFT_KEY is None:\n",
    "    raise ValueError(\"Configuration not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "This part is only for Google Colab. When the notebook is used locally we recommend using `poetry install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install \\\n",
    "    \"tenacity>=9.0.0,<10.0.0\" \\\n",
    "    \"tqdm>=4.67.1,<5.0.0\" \\\n",
    "    \"wordlift-sdk @ git+https://github.com/wordlift/python-sdk.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "This section provides general imports and basic configuration, no need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordlift_client import SitemapImportsApi, SitemapImportRequest, EmbeddingRequest\n",
    "from wordlift_sdk.client import ClientConfigurationFactory\n",
    "import wordlift_client\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from wordlift_sdk.utils import delayed, create_dataframe_of_url_id\n",
    "from tqdm.asyncio import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Awaitable\n",
    "import pandas as pd\n",
    "from wordlift_sdk.graphql import GraphQLClientFactory\n",
    "\n",
    "# Defining the host is optional and defaults to https://api.wordlift.io\n",
    "# See configuration.py for a list of all supported configuration parameters.\n",
    "api_url = \"https://api.wordlift.io\"\n",
    "configuration = ClientConfigurationFactory(key=WORDLIFT_KEY).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))\n",
    "async def import_url(url_list: list[str]) -> None:\n",
    "    import wordlift_client\n",
    "\n",
    "    async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "        imports_api = SitemapImportsApi(api_client)\n",
    "        request = SitemapImportRequest(\n",
    "            embedding=EmbeddingRequest(\n",
    "                properties=[\n",
    "                    \"http://schema.org/headline\",\n",
    "                    \"http://schema.org/abstract\",\n",
    "                    \"http://schema.org/text\",\n",
    "                ]\n",
    "            ),\n",
    "            output_types=[OUTPUT_TYPE],\n",
    "            urls=url_list,\n",
    "            overwrite=True,\n",
    "            id_generator=\"headline-with-url-hash\",\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            await imports_api.create_sitemap_import(sitemap_import_request=request)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error importing URLs: %s\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "\n",
    "This is the main notebook function code.\n",
    "\n",
    "## How does it work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from wordlift_client import AccountInfo, AnalysesResponse\n",
    "from pycountry import countries\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))\n",
    "async def process(row: Series) -> None:\n",
    "    url = row[\"url\"]\n",
    "    async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "        api_instance = wordlift_client.AnalyticsImportsApi(api_client)\n",
    "        request = AnalyticsImportRequest(urls=[url])\n",
    "        await api_instance.create_analytics_import(request)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EntityTopQuery:\n",
    "    iri: str\n",
    "    url: str\n",
    "    name: str\n",
    "    headline: str\n",
    "    title: str\n",
    "    top_query_iri: Optional[str] = field(default=None)\n",
    "    top_query_name: Optional[str] = field(default=None)\n",
    "    top_query_impressions: Optional[int] = field(default=None)\n",
    "    top_query_clicks: Optional[int] = field(default=None)\n",
    "    top_query_date_created: Optional[str] = field(default=None)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_graphql_response(entity_data: dict) -> \"EntityTopQuery\":\n",
    "        # Initialize top_query fields with default values\n",
    "        top_query_iri = top_query_name = top_query_impressions = top_query_clicks = (\n",
    "            top_query_date_created\n",
    "        ) = None\n",
    "\n",
    "        # Check if there are any top queries\n",
    "        if entity_data.get(\"top_query\"):\n",
    "            top_query_data = entity_data[\"top_query\"][0]\n",
    "            top_query_iri = top_query_data.get(\"iri\")\n",
    "            top_query_name = top_query_data.get(\"name\")\n",
    "            top_query_impressions = top_query_data.get(\"impressions\")\n",
    "            top_query_clicks = top_query_data.get(\"clicks\")\n",
    "            top_query_date_created = top_query_data.get(\"date_created\")\n",
    "\n",
    "        # Create an Entity instance\n",
    "        return EntityTopQuery(\n",
    "            iri=entity_data[\"iri\"],\n",
    "            url=entity_data[\"url\"],\n",
    "            name=entity_data[\"name\"],\n",
    "            headline=entity_data[\"headline\"],\n",
    "            title=entity_data[\"title\"],\n",
    "            top_query_iri=top_query_iri,\n",
    "            top_query_name=top_query_name,\n",
    "            top_query_impressions=top_query_impressions,\n",
    "            top_query_clicks=top_query_clicks,\n",
    "            top_query_date_created=top_query_date_created,\n",
    "        )\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))\n",
    "async def entity_with_top_query(url: str) -> Optional[EntityTopQuery]:\n",
    "    from gql import gql\n",
    "\n",
    "    # Create a GraphQL client using the defined transport\n",
    "    client = GraphQLClientFactory(key=WORDLIFT_KEY).create()\n",
    "\n",
    "    # Define the GraphQL query\n",
    "    gql_query = gql(\"\"\"\n",
    "        query($url: String!) {\n",
    "          entities(query: { urlConstraint: { in: [$url] } }) {\n",
    "            iri\n",
    "            url: string(name: \"schema:url\")\n",
    "            name: string(name: \"schema:name\")\n",
    "            headline: string(name: \"schema:headline\")\n",
    "            title: string(name: \"schema:title\")\n",
    "            top_query: topN(\n",
    "              name: \"seovoc:hasQuery\"\n",
    "              sort: { field: \"seovoc:impressions3Months\", direction: DESC }\n",
    "              limit: 1\n",
    "            ) {\n",
    "              iri\n",
    "              name: string(name: \"seovoc:name\")\n",
    "              impressions: int(name: \"seovoc:impressions3Months\")\n",
    "              clicks: int(name: \"seovoc:clicks3Months\")\n",
    "              date_created: date(name: \"seovoc:dateCreated\")\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    \"\"\")\n",
    "\n",
    "    # Asynchronous function to execute the query\n",
    "    async with client as session:\n",
    "        response = await session.execute(gql_query, variable_values={\"url\": url})\n",
    "\n",
    "        if len(response[\"entities\"]) == 0:\n",
    "            return None\n",
    "\n",
    "        return EntityTopQuery.from_graphql_response(response[\"entities\"][0])\n",
    "\n",
    "\n",
    "async def error_if_requirements_unsatisfied() -> Optional[AccountInfo]:\n",
    "    async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "        api = AccountApi(api_client)\n",
    "        account = await api.get_me()\n",
    "        if account.google_search_console_site_url is None:\n",
    "            logger.error(\n",
    "                \"%s is not connected to Google Search Console, open https://my.wordlift.io to connect it.\",\n",
    "                account.dataset_uri,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if account.country_code is None:\n",
    "            logger.error(\n",
    "                \"%s country code not configured, open https://my.wordlift.io to configure it.\",\n",
    "                account.dataset_uri,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        return account\n",
    "\n",
    "\n",
    "async def create_entities_with_top_query_dataframe(url_list: list[str]) -> DataFrame:\n",
    "    # Get the entities data with the top query.\n",
    "    logger.info(\"Loading entities with top query...\")\n",
    "    entities_with_top_query = await tqdm.gather(\n",
    "        *[delayed(entity_with_top_query, 4)(url) for url in url_list],\n",
    "        total=len(url_list),\n",
    "    )\n",
    "\n",
    "    entities_with_top_query_df = pd.DataFrame(entities_with_top_query)\n",
    "    entities_with_top_query_df[\"calc_name\"] = (\n",
    "        entities_with_top_query_df[[\"name\", \"headline\", \"title\", \"url\"]]\n",
    "        .bfill(axis=1)\n",
    "        .iloc[:, 0]\n",
    "    )\n",
    "    entities_with_top_query_df[\"top_query_date_created\"] = pd.to_datetime(\n",
    "        entities_with_top_query_df[\"top_query_date_created\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    return entities_with_top_query_df\n",
    "\n",
    "\n",
    "async def create_url_id_dataframe_importing_missing_urls(\n",
    "    key: str, url_list: list[str]\n",
    ") -> DataFrame:\n",
    "    url_id_df = await create_dataframe_of_url_id(key=key, url_list=url_list)\n",
    "    unique_urls = url_id_df[\"url\"].unique()\n",
    "\n",
    "    # Try importing the missing URLs.\n",
    "    while len(url_id_df) < len(URLS):\n",
    "        # Get the list of missing URLs, these are the URLs we'll import.\n",
    "        missing_url_list = list(set(URLS) - set(unique_urls))\n",
    "        logging.warning(\n",
    "            \"You provided %d URLs, %d URLs found in graph.\\nMissing URLs:\\n%s\",\n",
    "            len(URLS),\n",
    "            len(unique_urls),\n",
    "            \"\\n\".join(missing_url_list),\n",
    "        )\n",
    "\n",
    "        logger.info(\"Importing %d URLs\", len(missing_url_list))\n",
    "\n",
    "        # Import the URLs by calling the `import_url` method. We use `delayed` to parallelize work.\n",
    "        await tqdm.gather(\n",
    "            *[delayed(import_url)([url]) for url in missing_url_list],\n",
    "            total=len(missing_url_list),\n",
    "        )\n",
    "\n",
    "        url_id_df = await create_dataframe_of_url_id(key=WORDLIFT_KEY, url_list=URLS)\n",
    "\n",
    "    return url_id_df\n",
    "\n",
    "\n",
    "async def create_entity_gaps_factory(\n",
    "    query_location_name: str,\n",
    ") -> Callable[[Series], Awaitable[Optional[AnalysesResponse]]]:\n",
    "    @retry(stop=stop_after_attempt(10), wait=wait_fixed(2))\n",
    "    async def create_entity_gaps(row: Series) -> Optional[AnalysesResponse]:\n",
    "        url = row[\"url\"]\n",
    "        query = row[\"top_query_name\"]\n",
    "        if query is None:\n",
    "            return None\n",
    "\n",
    "        async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "            api = EntityGapsApi(api_client)\n",
    "            return await api.create_entity_gap(\n",
    "                EntityGapRequest(\n",
    "                    url=url, query=query, query_location_name=query_location_name\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return create_entity_gaps\n",
    "\n",
    "\n",
    "async def append_entity_gaps_response_to_row_factory(\n",
    "    create_entity_gaps: Callable[[Series], Awaitable[Optional[AnalysesResponse]]],\n",
    ") -> Callable[[Series], Awaitable[Series]]:\n",
    "    async def append_entity_gaps_response_to_row(row: Series) -> Series:\n",
    "        response = await create_entity_gaps(row)\n",
    "        if response:\n",
    "            row[\"entity_gaps\"] = response.items\n",
    "        return row\n",
    "\n",
    "    return append_entity_gaps_response_to_row\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    # Exit if Google Search Console isn't connected.\n",
    "    account = await error_if_requirements_unsatisfied()\n",
    "    if account is None:\n",
    "        return\n",
    "\n",
    "    # Get the country name\n",
    "    country = countries.get(alpha_2=account.country_code.upper())\n",
    "    if country is None:\n",
    "        logger.error(\n",
    "            \"Country code %s is invalid, open https://my.wordlift.io to reconfigure it.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Get the list of URLs in the dataframe.\n",
    "    url_id_df = await create_url_id_dataframe_importing_missing_urls(\n",
    "        key=WORDLIFT_KEY, url_list=URLS\n",
    "    )\n",
    "    unique_urls = url_id_df[\"url\"].unique()\n",
    "\n",
    "    # Get the entities data with the top query.\n",
    "    entities_with_top_query_df = await create_entities_with_top_query_dataframe(\n",
    "        unique_urls\n",
    "    )\n",
    "\n",
    "    # Calculate the date 7 days ago from today\n",
    "    seven_days_ago = datetime.now() - timedelta(days=7)\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    entities_with_stale_data_df = entities_with_top_query_df[\n",
    "        entities_with_top_query_df[\"top_query_date_created\"].isna()\n",
    "        | (entities_with_top_query_df[\"top_query_date_created\"] < seven_days_ago)\n",
    "    ]\n",
    "\n",
    "    if len(entities_with_stale_data_df) > 0:\n",
    "        logger.info(\"Updating missing or stale Google Search Console data...\")\n",
    "        # We're polite and not making more than 2 concurrent reqs.\n",
    "        await tqdm.gather(\n",
    "            *[\n",
    "                delayed(process, 2)(row)\n",
    "                for index, row in entities_with_stale_data_df.iterrows()\n",
    "            ],\n",
    "            total=len(entities_with_stale_data_df),\n",
    "        )\n",
    "\n",
    "        entities_with_top_query_df = await create_entities_with_top_query_dataframe(\n",
    "            unique_urls\n",
    "        )\n",
    "\n",
    "    await tqdm.gather(\n",
    "        *[\n",
    "            delayed(\n",
    "                await append_entity_gaps_response_to_row_factory(\n",
    "                    create_entity_gaps=await create_entity_gaps_factory(country.name)\n",
    "                ),\n",
    "                2,\n",
    "            )(row)\n",
    "            for index, row in entities_with_top_query_df.iterrows()\n",
    "        ],\n",
    "        total=len(entities_with_top_query_df),\n",
    "    )\n",
    "\n",
    "    [\n",
    "        logger.info(\"%s, top query '%s':\", row[\"calc_name\"], row[\"top_query_name\"])\n",
    "        for index, row in entities_with_top_query_df.iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
