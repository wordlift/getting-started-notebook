{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Google Search Console data\n",
    "\n",
    "## Configuration\n",
    "\n",
    "There are two configuration sources, at least one of the two is needed, and they're applied in order:\n",
    "\n",
    "1. A file config/default.py\n",
    "2. Local constants and WordLift Key in Google Colab Secrets\n",
    "\n",
    "There's only one configuration settings:\n",
    "\n",
    "* `WORDLIFT_KEY`, holding the WordLift Key, when using Google Colab, it can be set in the secrets"
   ],
   "id": "2685a42ac518ed8a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import logging\n",
    "\n",
    "from wordlift_client import AnalyticsImportRequest\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Configuration from config/default.py file.\n",
    "try:\n",
    "    # Configuration is in the `config/default.py` file.\n",
    "    from config import default as config\n",
    "\n",
    "    WORDLIFT_KEY = config.WORDLIFT_KEY\n",
    "    OUTPUT_TYPE = config.OUTPUT_TYPE or 'http://schema.org/WebPage'\n",
    "    URLS = config.URLS\n",
    "except ImportError:\n",
    "    logging.warning(\"Cannot import configuration from local `config/default.py` file.\")\n",
    "\n",
    "# Configuration from Google Colab Secrets.\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    WORDLIFT_KEY = userdata.get('WORDLIFT_KEY')\n",
    "    OUTPUT_TYPE = 'http://schema.org/WebPage'\n",
    "    URLS = []\n",
    "except ImportError:\n",
    "    logging.warning(\"Cannot import configuration from google.colab.usermap.\")\n",
    "\n",
    "if WORDLIFT_KEY is None:\n",
    "    raise ValueError('Configuration not set')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dependencies\n",
    "\n",
    "This part is only for Google Colab. When the notebook is used locally we recommend using `poetry install`."
   ],
   "id": "4ee46cce8dbbfb92"
  },
  {
   "cell_type": "code",
   "id": "77d619a97e7cdba",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install \\\n",
    "    \"tenacity>=9.0.0,<10.0.0\" \\\n",
    "    \"tqdm>=4.67.1,<5.0.0\" \\\n",
    "    \"wordlift-sdk @ git+https://github.com/wordlift/python-sdk.git\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "\n",
    "This section provides general imports and basic configuration, no need to do anything here."
   ],
   "id": "2a6a1b27996b66e2"
  },
  {
   "cell_type": "code",
   "id": "c68f36502e2d0448",
   "metadata": {},
   "source": [
    "\n",
    "from wordlift_client import SitemapImportsApi, SitemapImportRequest, EmbeddingRequest\n",
    "from wordlift_sdk.client import ClientConfigurationFactory\n",
    "import wordlift_client\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from wordlift_sdk.utils import delayed, create_dataframe_of_url_id\n",
    "from tqdm.asyncio import tqdm\n",
    "from pandas import Series\n",
    "\n",
    "# Defining the host is optional and defaults to https://api.wordlift.io\n",
    "# See configuration.py for a list of all supported configuration parameters.\n",
    "api_url = 'https://api.wordlift.io'\n",
    "configuration = ClientConfigurationFactory(key=WORDLIFT_KEY).create()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_fixed(2)\n",
    ")\n",
    "async def import_url(url_list: list[str]) -> None:\n",
    "    import wordlift_client\n",
    "\n",
    "    async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "        imports_api = SitemapImportsApi(api_client)\n",
    "        request = SitemapImportRequest(\n",
    "            embedding=EmbeddingRequest(\n",
    "                properties=[\"http://schema.org/headline\", \"http://schema.org/abstract\", \"http://schema.org/text\"]\n",
    "            ),\n",
    "            output_types=[OUTPUT_TYPE],\n",
    "            urls=url_list,\n",
    "            overwrite=True,\n",
    "            id_generator=\"headline-with-url-hash\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            await imports_api.create_sitemap_import(sitemap_import_request=request)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error importing URLs: %s\", e)"
   ],
   "id": "d757a15050e3b6bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main Function\n",
    "\n",
    "This is the main notebook function code.\n",
    "\n",
    "## How does it work\n",
    "\n"
   ],
   "id": "fa1efeeb53d4c8ce"
  },
  {
   "cell_type": "code",
   "id": "6310f73849a7abf3",
   "metadata": {},
   "source": [
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_fixed(2)\n",
    ")\n",
    "async def process(url: str) -> None:\n",
    "    async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "        api_instance = wordlift_client.AnalyticsImportsApi(api_client)\n",
    "        request = AnalyticsImportRequest(urls=[url])\n",
    "        await api_instance.create_analytics_import(request)\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    url_id_df = await create_dataframe_of_url_id(key=WORDLIFT_KEY, url_list=URLS)\n",
    "    unique_urls = url_id_df['url'].unique()\n",
    "\n",
    "    while len(url_id_df) < len(URLS):\n",
    "        # Get the list of missing URLs, these are the URLs we'll import.\n",
    "        missing_url_list = list(set(URLS) - set(unique_urls))\n",
    "        logging.warning(\n",
    "            \"You provided %d URLs, %d URLs found in graph.\\nMissing URLs:\\n%s\",\n",
    "            len(URLS),\n",
    "            len(unique_urls),\n",
    "            '\\n'.join(missing_url_list)\n",
    "        )\n",
    "\n",
    "        logging.info(\"Importing %d URLs\", len(missing_url_list))\n",
    "\n",
    "        # Import the URLs by calling the `import_url` method. We use `delayed` to parallelize work.\n",
    "        await tqdm.gather(*[delayed(import_url)([url]) for url in missing_url_list], total=len(missing_url_list))\n",
    "\n",
    "        url_id_df = await create_dataframe_of_url_id(key=WORDLIFT_KEY, url_list=URLS)\n",
    "\n",
    "    logging.info(\"Loading Google Search Console data\")\n",
    "    # We're polite and not making more than 2 concurrent reqs.\n",
    "    await tqdm.gather(\n",
    "        *[delayed(process, 2)(url) for url in unique_urls],\n",
    "        total=len(url_id_df)\n",
    "    )\n",
    "\n",
    "\n",
    "await main()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
