{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Jumpstart a Graph in a few steps with WordLift by providing an XML sitemap and a WordLift Key ([get it here](https://wordlift.io)).\n",
    "\n",
    "This is the first notebook of a series, once you created your graph, move forward to [create internal links](create_internal_links.ipynb).\n",
    "\n",
    "## Video Walkthrough\n",
    "\n",
    "[![Jumpstart your Graph in less than 5 minutes](https://img.youtube.com/vi/yQV9DkH9LmI/0.jpg)](https://www.youtube.com/watch?v=yQV9DkH9LmI)\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Configuration combines 4 sources in order:\n",
    "\n",
    "1. Global keys found in `globals()` allowing to use this notebook with JupyterLab Scheduler which presets the configurations as globals.\n",
    "2. Keys from a local configuration file, by default `config/default.py`\n",
    "3. Environment variables.\n",
    "4. Google Colab usermap (i.e. secrets).\n",
    "\n",
    "There are three configuration settings:\n",
    "\n",
    "* `WORDLIFT_KEY`, holding the WordLift Key, when using Google Colab, it can be set in the secrets\n",
    "* `OUTPUT_TYPE`, optional, this is the type used to represent imported web pages. If not set will default to `http://schema.org/WebPage` (other options could be `http://schema.org/CollectionPage`, etc.)\n",
    "* `SITEMAP_URL`, the URL to the sitemap which contains URLs (not other sitemaps, or at least we didn't test it with links to other sitemaps)\n",
    "* alternative provide a `SHEETS_URL` and `SHEETS_NAME` configuration to read the list of URLs from the `url` column of the specified Google Sheets spreadsheet, this will require a valid [Google service account](./docs/create_google_service_account.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from wordlift_sdk.config import get_config_value\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, force=True)\n",
    "\n",
    "# Suppress all other loggers below WARNING\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name != __name__:\n",
    "        logging.getLogger(name).setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Try to read the configuration from the `config/default.py` file.\n",
    "config_path = Path.cwd() / \"config\" / \"default.py\"\n",
    "WORDLIFT_KEY = get_config_value(\"WORDLIFT_KEY\", config_path)\n",
    "SITEMAP_URL = get_config_value(\"SITEMAP_URL\", config_path)\n",
    "OUTPUT_TYPES = {\n",
    "    get_config_value(\"OUTPUT_TYPE\", config_path, \"http://schema.org/Article\")\n",
    "}\n",
    "SHEETS_URL = get_config_value(\"SHEETS_URL\", config_path)\n",
    "SHEETS_NAME = get_config_value(\"SHEETS_NAME\", config_path)\n",
    "SHEETS_SERVICE_ACCOUNT = get_config_value(\"SHEETS_SERVICE_ACCOUNT\", config_path)\n",
    "URLS = get_config_value(\"URLS\", config_path)\n",
    "INTERNAL_LINKS = get_config_value(\"INTERNAL_LINKS\", config_path, False)\n",
    "\n",
    "if WORDLIFT_KEY is None:\n",
    "    raise ValueError(\"`WORDLIFT_KEY` is required.\")\n",
    "\n",
    "if OUTPUT_TYPES is None:\n",
    "    raise ValueError(\"`OUTPUT_TYPES` is required.\")\n",
    "\n",
    "if (\n",
    "    SITEMAP_URL is None\n",
    "    and URLS is None\n",
    "    and (SHEETS_URL is None or SHEETS_NAME is None or SHEETS_SERVICE_ACCOUNT is None)\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"One of `SITEMAP_URL` or `SHEETS_URL`/`SHEETS_NAME`/`SHEETS_SERVICE_ACCOUNT` is required.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "This part is only for Google Colab. When the notebook is used locally we recommend using `poetry install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordlift_sdk.notebook import install_if_missing\n",
    "\n",
    "install_if_missing(\"wordlift-client>=1.75.0,<2.0.0)\", \"wordlift_client\")\n",
    "install_if_missing(\"beautifulsoup4>=4.13.3,<5.0.0)\", \"bs4\")\n",
    "install_if_missing(\"rdflib>=7.1.3,<8.0.0)\", \"rdflib\")\n",
    "install_if_missing(\"tenacity>=9.0.0,<10.0.0)\", \"tenacity\")\n",
    "install_if_missing(\"pycountry>=24.6.1,<25.0.0)\", \"pycountry\")\n",
    "install_if_missing(\n",
    "    \"wordlift-sdk @ git+https://github.com/wordlift/python-sdk.git@0.34.1\",\n",
    "    \"wordlift_sdk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "This section provides general imports and basic configuration, no need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordlift_sdk.client import ClientConfigurationFactory\n",
    "\n",
    "# Defining the host is optional and defaults to https://api.wordlift.io\n",
    "# See configuration.py for a list of all supported configuration parameters.\n",
    "api_url = \"https://api.wordlift.io\"\n",
    "configuration = ClientConfigurationFactory(key=WORDLIFT_KEY).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "There are two callbacks that you can customize according to your needs:\n",
    "\n",
    "1. `import_url`, imports a URL into the Graph, it is called for each URL found in the sitemap.\n",
    "2. `parse_html`, parses the webpage and provides a list of entity patches to add additional properties to the imported entities and is called for every url.\n",
    "\n",
    "## Import URL\n",
    "\n",
    "The defaults work nicely for most situations so that you don't really need to configure anything here. This is the default callback from the SDK:\n",
    "\n",
    "```python\n",
    "@@TODO\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Parse HTML\n",
    "\n",
    "This example shows how to add `schema:keywords` to an imported entity by taking the values from the `<a class=\"tag-cloud-link\">Tag</a>` markup. You can further tailor this part based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Create Internal Links\n",
    "\n",
    "This method is called by the `main` method in order to create the Internal Links.\n",
    "\n",
    "## How does it work\n",
    "\n",
    "We query the graph for all the entities that have embedding vectors, the results are stored in `iri`, `url` pairs in the `entities_with_embedding_vectors_df` dataframe.\n",
    "\n",
    "We then use the SDK's `create_internal_link_handler` method to pass the Client configuration and the ID of the Link Group that we want to create.\n",
    "\n",
    "We can optionally provide an `internal_link_request_filter` method with the following signature `Callable[[Series, InternalLinkRequest], Awaitable[InternalLinkRequest]` to alter the actual request with additional filters (for example we may want to filter by at least a matching keyword shared between the source web page and the target web page).\n",
    "\n",
    "The results are going to be written to the graph using schema.org and [seontology](https://github.com/seontology/seontology/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "from wordlift_sdk.internal_link import create_internal_link_handler\n",
    "from wordlift_sdk.utils import (\n",
    "    create_dataframe_of_entities_with_embedding_vectors,\n",
    "    delayed,\n",
    ")\n",
    "from pandas import DataFrame\n",
    "from wordlift_client import Configuration\n",
    "\n",
    "\n",
    "async def create_internal_links(configuration: Configuration, key: str) -> DataFrame:\n",
    "    entities_with_embedding_vectors_df = (\n",
    "        await create_dataframe_of_entities_with_embedding_vectors(key)\n",
    "    )\n",
    "\n",
    "    # We're polite and not making more than 2 concurrent reqs.\n",
    "    handler = create_internal_link_handler(configuration, \"getting_started\")\n",
    "    await tqdm.gather(\n",
    "        *[\n",
    "            delayed(handler, 2)(row)\n",
    "            for index, row in entities_with_embedding_vectors_df.iterrows()\n",
    "        ],\n",
    "        total=len(entities_with_embedding_vectors_df),\n",
    "    )\n",
    "\n",
    "    return entities_with_embedding_vectors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "\n",
    "This is the main notebook function code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordlift_sdk.google_search_console import (\n",
    "    raise_error_if_account_analytics_not_configured,\n",
    "    create_google_search_console_data_import,\n",
    ")\n",
    "from wordlift_sdk.utils import get_me\n",
    "from wordlift_sdk.wordlift.sitemap_import.create_or_update_kg import (\n",
    "    create_or_update_kg_using_url_provider,\n",
    ")\n",
    "import gspread\n",
    "from wordlift_sdk.kg.manager.urlprovider import (\n",
    "    UrlProviderFactory,\n",
    "    UrlProviderFactoryInput,\n",
    ")\n",
    "from wordlift_sdk.wordlift.sitemap_import.protocol import (\n",
    "    ProtocolContext,\n",
    "    load_override_class,\n",
    "    DefaultImportUrlProtocol,\n",
    "    DefaultParseHtmlProtocol,\n",
    ")\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    # Define the context for protocol functions, it provides instances and data that those functions can use.\n",
    "    protocol_context = ProtocolContext(\n",
    "        configuration=configuration,\n",
    "        types=OUTPUT_TYPES,\n",
    "    )\n",
    "\n",
    "    # Change the behavior of the import URL call.\n",
    "    import_url_protocol = load_override_class(\n",
    "        name=\"import_url_protocol\",\n",
    "        class_name=\"ImportUrlProtocol\",\n",
    "        # Default class to use in case of missing override.\n",
    "        default_class=DefaultImportUrlProtocol,\n",
    "        context=protocol_context,\n",
    "    )\n",
    "\n",
    "    # Change the behavior of the parse HTML call by parsing the web page html and sending patches to the graph.\n",
    "    parse_html_protocol = load_override_class(\n",
    "        name=\"parse_html_protocol\",\n",
    "        class_name=\"ParseHtmlProtocol\",\n",
    "        # Default class to use in case of missing override.\n",
    "        default_class=DefaultParseHtmlProtocol,\n",
    "        context=protocol_context,\n",
    "    )\n",
    "\n",
    "    # We can import for different sources: (1) sitemap or (2) Google Sheets or (3) list of URLs.\n",
    "    # This will create the provider based on the configuration and the input parameters.\n",
    "    url_provider = UrlProviderFactory.create(\n",
    "        input_params=UrlProviderFactoryInput(\n",
    "            sitemap_url=SITEMAP_URL,\n",
    "            sheets_url=SHEETS_URL,\n",
    "            sheets_name=SHEETS_NAME,\n",
    "            sheets_creds_or_client=gspread.service_account(\n",
    "                filename=SHEETS_SERVICE_ACCOUNT\n",
    "            )\n",
    "            if SHEETS_SERVICE_ACCOUNT\n",
    "            else None,\n",
    "            urls=URLS,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Import URLs.\n",
    "    logger.info(\"Importing...\")\n",
    "    await create_or_update_kg_using_url_provider(\n",
    "        configuration=configuration,\n",
    "        key=WORDLIFT_KEY,\n",
    "        url_provider=url_provider,\n",
    "        types=OUTPUT_TYPES,\n",
    "        concurrency=1,\n",
    "        import_url_protocol=import_url_protocol,\n",
    "        parse_html_protocol=parse_html_protocol,\n",
    "    )\n",
    "\n",
    "    # Check if we can import analytics.\n",
    "    account = await get_me(configuration=configuration)\n",
    "    try:\n",
    "        has_analytics = await raise_error_if_account_analytics_not_configured(\n",
    "            account=account\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        has_analytics = False\n",
    "        logger.error(e)\n",
    "\n",
    "    if has_analytics:\n",
    "        url_list = set()\n",
    "        async for url in url_provider.urls():\n",
    "            url_list.add(url.value)\n",
    "\n",
    "        logger.info(\n",
    "            \"Importing Google Search Console data for %d URLs: (1) load existing data and (2) create/update data...\",\n",
    "            len(url_list),\n",
    "        )\n",
    "        await create_google_search_console_data_import(\n",
    "            configuration=configuration,\n",
    "            key=WORDLIFT_KEY,\n",
    "            url_list=url_list,\n",
    "        )\n",
    "\n",
    "        # logger.info(\n",
    "        #     \"Creating entity gaps data %d URLs...\",\n",
    "        #     len(url_list),\n",
    "        # )\n",
    "        # await create_entity_gaps(\n",
    "        #     configuration=configuration,\n",
    "        #     key=WORDLIFT_KEY,\n",
    "        #     account=account,\n",
    "        #     url_list=url_list\n",
    "        # )\n",
    "\n",
    "    if INTERNAL_LINKS:\n",
    "        entities_with_embedding_vectors_df = await create_internal_links(\n",
    "            configuration=configuration, key=WORDLIFT_KEY\n",
    "        )\n",
    "\n",
    "        # Print the ID of the entities processed\n",
    "        for index, row in entities_with_embedding_vectors_df.iterrows():\n",
    "            logger.info(row[\"url\"] + \" \" + row[\"iri\"])\n",
    "\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
