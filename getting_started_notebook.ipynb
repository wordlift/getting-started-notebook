{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Onboarding Notebook\n",
    "\n",
    "The aim is to import webpages by providing a list of URLs. The list may come from any source, typically from a sitemap.\n",
    "\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-27T17:29:53.517113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "import advertools as adv\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import wordlift_client\n",
    "from aiohttp import ClientSession\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import Series\n",
    "from rdflib import URIRef, Literal\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from tqdm.asyncio import tqdm\n",
    "from wordlift_client import SitemapImportsApi, SitemapImportRequest, EmbeddingRequest, ApiException\n",
    "from wordlift_sdk.graphql import GraphQLClientFactory\n",
    "from wordlift_sdk.utils import create_entity_patch_request\n",
    "from wordlift_sdk.client import ClientConfigurationFactory\n",
    "\n",
    "# Configuration is in the `config/default.py` file.\n",
    "from config import default as config\n",
    "\n",
    "logging.basicConfig(level=logging.WARN, force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Defining the host is optional and defaults to https://api.wordlift.io\n",
    "# See configuration.py for a list of all supported configuration parameters.\n",
    "api_url = 'https://api.wordlift.io'\n",
    "output_type = config.OUTPUT_TYPE or 'http://schema.org/WebPage'\n",
    "configuration = ClientConfigurationFactory(key=config.WORDLIFT_KEY).create()\n",
    "\n",
    "\n",
    "async def kg():\n",
    "    from gql import gql\n",
    "\n",
    "    # Create a GraphQL client using the defined transport\n",
    "    client = GraphQLClientFactory(key=config.WORDLIFT_KEY).create()\n",
    "\n",
    "    # Define the GraphQL query\n",
    "    query = gql(\"\"\"\n",
    "        query getEntities($type_constraint: String!) {\n",
    "          entities(\n",
    "            query: { typeConstraint: { in: [$type_constraint] } }\n",
    "          ) {\n",
    "            id: iri\n",
    "            keywords: string(name: \"schema:keywords\")\n",
    "            url: string(name: \"schema:url\")\n",
    "          }\n",
    "        }\n",
    "    \"\"\")\n",
    "\n",
    "    variables = {\n",
    "        \"type_constraint\": output_type\n",
    "    }\n",
    "\n",
    "    # Asynchronous function to execute the query\n",
    "    async with client as session:\n",
    "        response = await session.execute(query, variable_values=variables)\n",
    "        return pd.DataFrame(response['entities'], columns=['id', 'keywords', 'url'])\n",
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),  # Retry up to 5 times\n",
    "    wait=wait_fixed(2)  # Wait 2 seconds between retries\n",
    ")\n",
    "async def enrich(row: Series):\n",
    "    async def fetch(session: ClientSession, url: str) -> str:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "    async def parse(html: str) -> [str]:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Initialize an empty list to hold the combined results\n",
    "        combined_content = []\n",
    "\n",
    "        # Extract the 'mz:section' meta tag\n",
    "        section_meta = soup.find('meta', attrs={'property': 'mz:section'})\n",
    "        if section_meta and 'content' in section_meta.attrs:\n",
    "            section_content = [item.strip() for item in section_meta['content'].split(',')]\n",
    "            combined_content.extend(section_content)\n",
    "\n",
    "        # Extract the 'mz:subsection' meta tag\n",
    "        subsection_meta = soup.find('meta', attrs={'property': 'mz:subsection'})\n",
    "        if subsection_meta and 'content' in subsection_meta.attrs:\n",
    "            subsection_content = [item.strip() for item in subsection_meta['content'].split(',')]\n",
    "            combined_content.extend(subsection_content)\n",
    "\n",
    "        return combined_content\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        entity_url = row['url']\n",
    "        entity_id = row['id']\n",
    "        html = await fetch(session, entity_url)\n",
    "        keywords = await parse(html)\n",
    "\n",
    "        logger.info(f\"Processing entity: {entity_id} for URL: {entity_url}\")\n",
    "        resource = URIRef(entity_id)\n",
    "\n",
    "        payloads = []\n",
    "\n",
    "        for value in keywords:\n",
    "            payloads.append(\n",
    "                create_entity_patch_request(\n",
    "                    resource,\n",
    "                    URIRef('http://schema.org/keywords'),\n",
    "                    Literal(value)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # If the payloads are empty, exit.\n",
    "        if not payloads:\n",
    "            return\n",
    "\n",
    "        async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "            api_instance = wordlift_client.EntitiesApi(api_client)\n",
    "\n",
    "            logger.info(f\"Created {len(payloads)} patch requests for entity {entity_id}\")\n",
    "\n",
    "            try:\n",
    "                logger.info(f\"Sending patch request for entity {entity_id}\")\n",
    "                api_response = await api_instance.patch_entities(entity_id, payloads)\n",
    "                logger.info(f\"Successfully added attributes to entity {entity_id}\")\n",
    "                logger.debug(f\"API response: {api_response}\")\n",
    "            except ApiException as e:\n",
    "                logger.error(f\"ApiException when calling EntitiesApi->patch_entities: {e}\")\n",
    "                logger.error(f\"Response body: {e.body}\")\n",
    "                logger.error(f\"Response headers: {e.headers}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected exception when calling EntitiesApi->patch_entities: {e}\")\n",
    "\n",
    "            logger.info(f\"Finished processing entity: {entity_id}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    import asyncio\n",
    "    from os import cpu_count\n",
    "\n",
    "    def delayed(callback, concurrency=cpu_count() + 1):\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "        async def callback_with_semaphore(row):\n",
    "            async with sem:\n",
    "                return await callback(row)\n",
    "\n",
    "        return callback_with_semaphore\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(5),\n",
    "        wait=wait_fixed(2)\n",
    "    )\n",
    "    async def import_url(url_list: list[str]) -> None:\n",
    "        import wordlift_client\n",
    "\n",
    "        async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "            imports_api = SitemapImportsApi(api_client)\n",
    "            request = SitemapImportRequest(\n",
    "                embedding=EmbeddingRequest(\n",
    "                    properties=[\"http://schema.org/headline\", \"http://schema.org/abstract\", \"http://schema.org/text\"]\n",
    "                ),\n",
    "                output_types=[output_type],\n",
    "                urls=url_list,\n",
    "                overwrite=True,\n",
    "                id_generator=\"headline-with-url-hash\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                await imports_api.create_sitemap_import(sitemap_import_request=request)\n",
    "            except Exception as e:\n",
    "                logger.error(\"Error importing URLs: %s\", e)\n",
    "\n",
    "    sitemap_df = adv.sitemap_to_df(config.SITEMAP_URL)\n",
    "    kg_df = await kg()\n",
    "    missing_url_list = list(set(sitemap_df['loc']) - set(kg_df['url']))\n",
    "    callback = delayed(import_url)\n",
    "    await tqdm.gather(*[callback([url]) for url in missing_url_list], total=len(missing_url_list))\n",
    "\n",
    "    no_keywords_df = kg_df[kg_df['keywords'].isna()]\n",
    "    await tqdm.gather(\n",
    "        *[delayed(enrich)(row) for index, row in no_keywords_df.iterrows()],\n",
    "        total=len(no_keywords_df)\n",
    "    )\n",
    "\n",
    "\n",
    "await main()\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
      " 95%|█████████▌| 39/41 [00:17<00:00, 17.77it/s]"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
