{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Onboarding Notebook\n",
    "\n",
    "The aim is to import webpages by providing a list of URLs. The list may come from any source, typically from a sitemap.\n",
    "\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install \\\n",
    "    \"advertools>=0.16.4,<0.17.0\" \\\n",
    "    \"wordlift-client>=1.75.0,<2.0.0\" \\\n",
    "    \"tqdm>=4.67.1,<5.0.0\" \\\n",
    "    \"gql[aiohttp]>=3.5.1,<4.0.0\" \\\n",
    "    \"beautifulsoup4>=4.13.3,<5.0.0\" \\\n",
    "    \"rdflib>=7.1.3,<8.0.0\" \\\n",
    "    \"tenacity>=9.0.0,<10.0.0\" \\\n",
    "    \"wordlift-sdk @ git+https://github.com/wordlift/python-sdk.git\"\n"
   ],
   "id": "29ae0702e933346",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import advertools as adv\n",
    "import wordlift_sdk.entity as entity\n",
    "import wordlift_sdk.graphql as graphql\n",
    "from bs4 import BeautifulSoup\n",
    "from rdflib import URIRef, Literal\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from tqdm.asyncio import tqdm\n",
    "from wordlift_client import SitemapImportsApi, SitemapImportRequest, EmbeddingRequest, EntityPatchRequest\n",
    "from wordlift_sdk.client import ClientConfigurationFactory\n",
    "from wordlift_sdk.utils import create_entity_patch_request, delayed\n",
    "\n",
    "try:\n",
    "    # Configuration is in the `config/default.py` file.\n",
    "    from config import default as config\n",
    "except ImportError:\n",
    "    logging.warn(\"Cannot import configuration from local `config/default.py` file.\")\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    WORDLIFT_KEY = userdata.get('WORDLIFT_KEY')\n",
    "    OUTPUT_TYPE = 'http://schema.org/WebPage'\n",
    "    SITEMAP_URL = '...set here the sitemap url...'\n",
    "except ImportError:\n",
    "    logging.warn(\"Cannot import configuration from google.colab.usermap.\")\n",
    "\n",
    "logging.basicConfig(level=logging.WARN, force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Defining the host is optional and defaults to https://api.wordlift.io\n",
    "# See configuration.py for a list of all supported configuration parameters.\n",
    "api_url = 'https://api.wordlift.io'\n",
    "output_type = config.OUTPUT_TYPE or 'http://schema.org/WebPage'\n",
    "configuration = ClientConfigurationFactory(key=config.WORDLIFT_KEY).create()\n",
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_fixed(2)\n",
    ")\n",
    "async def import_url(url_list: list[str]) -> None:\n",
    "    import wordlift_client\n",
    "\n",
    "    async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "        imports_api = SitemapImportsApi(api_client)\n",
    "        request = SitemapImportRequest(\n",
    "            embedding=EmbeddingRequest(\n",
    "                properties=[\"http://schema.org/headline\", \"http://schema.org/abstract\", \"http://schema.org/text\"]\n",
    "            ),\n",
    "            output_types=[output_type],\n",
    "            urls=url_list,\n",
    "            overwrite=True,\n",
    "            id_generator=\"headline-with-url-hash\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            await imports_api.create_sitemap_import(sitemap_import_request=request)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error importing URLs: %s\", e)\n",
    "\n",
    "\n",
    "async def parse_html(entity_id: str, html: str) -> list[EntityPatchRequest]:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Initialize an empty list to hold the combined results\n",
    "    combined_content = []\n",
    "\n",
    "    # Extract the 'mz:section' meta tag\n",
    "    section_meta = soup.find('meta', attrs={'property': 'mz:section'})\n",
    "    if section_meta and 'content' in section_meta.attrs:\n",
    "        section_content = [item.strip() for item in section_meta['content'].split(',')]\n",
    "        combined_content.extend(section_content)\n",
    "\n",
    "    # Extract the 'mz:subsection' meta tag\n",
    "    subsection_meta = soup.find('meta', attrs={'property': 'mz:subsection'})\n",
    "    if subsection_meta and 'content' in subsection_meta.attrs:\n",
    "        subsection_content = [item.strip() for item in subsection_meta['content'].split(',')]\n",
    "        combined_content.extend(subsection_content)\n",
    "\n",
    "    resource = URIRef(entity_id)\n",
    "\n",
    "    payloads = []\n",
    "\n",
    "    for value in combined_content:\n",
    "        payloads.append(\n",
    "            create_entity_patch_request(\n",
    "                resource,\n",
    "                URIRef('http://schema.org/keywords'),\n",
    "                Literal(value)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return payloads\n",
    "\n",
    "\n",
    "async def kg() -> pd.DataFrame:\n",
    "    return await graphql.query(\n",
    "        key=config.WORDLIFT_KEY,\n",
    "        query_string=\"\"\"\n",
    "            query getEntities($type_constraint: String!) {\n",
    "              entities(\n",
    "                query: { typeConstraint: { in: [$type_constraint] } }\n",
    "              ) {\n",
    "                id: iri\n",
    "                keywords: string(name: \"schema:keywords\")\n",
    "                url: string(name: \"schema:url\")\n",
    "              }\n",
    "            }\n",
    "        \"\"\",\n",
    "        root_element=\"entities\",\n",
    "        columns=['id', 'keywords', 'url'],\n",
    "        variable_values={\n",
    "            \"type_constraint\": output_type\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Get the list of URLs from the sitemap (`loc` column)\n",
    "    sitemap_df = adv.sitemap_to_df(config.SITEMAP_URL)\n",
    "\n",
    "    # Get the data from the KG to determine which URLs are already imported and which not.\n",
    "    kg_df = kg()\n",
    "\n",
    "    # Get the list of missing URLs, these are the URLs we'll import.\n",
    "    missing_url_list = list(set(sitemap_df['loc']) - set(kg_df['url']))\n",
    "\n",
    "    # Import the URLs by calling the `import_url` method. We use `delayed` to parallelize work.\n",
    "    await tqdm.gather(*[delayed(import_url)([url]) for url in missing_url_list], total=len(missing_url_list))\n",
    "\n",
    "    # Reload the Kg after the import to get the list of URLs that are missing the `keywords` field.\n",
    "    # @@TODO we can call a different graphql query that filters already by keywords not present or empty instead of filtering client-side.\n",
    "    kg_df = kg()\n",
    "\n",
    "    # Filter the KG to list only the URLs without `keywords`.\n",
    "    no_keywords_df = kg_df[kg_df['keywords'].isna()]\n",
    "\n",
    "    # Enrich the Graph, notice that here we pass our callback `parse_html` which will return Patch requests, no need to deal with the actual API.\n",
    "    await tqdm.gather(\n",
    "        *[delayed(entity.enrich(configuration, parse_html))(row) for index, row in no_keywords_df.iterrows()],\n",
    "        total=len(no_keywords_df)\n",
    "    )\n",
    "\n",
    "\n",
    "await main()\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
