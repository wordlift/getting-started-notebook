{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Onboarding Notebook\n",
    "\n",
    "The aim is to import webpages by providing a list of URLs. The list may come from any source, typically from a sitemap.\n",
    "\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-27T16:59:57.131416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from wordlift_sdk.utils import create_entity_patch_request\n",
    "from rdflib import URIRef, SDO, Literal\n",
    "from pandas import Series\n",
    "from aiohttp import ClientSession\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "import advertools as adv\n",
    "import pandas as pd\n",
    "import wordlift_client\n",
    "from tqdm.asyncio import tqdm\n",
    "from wordlift_client import SitemapImportsApi, SitemapImportRequest, EmbeddingRequest, ApiException\n",
    "from wordlift_sdk.graphql import GraphQLClientFactory\n",
    "\n",
    "# Configuration is in the `config/default.py` file.\n",
    "from config import default as config\n",
    "\n",
    "logging.basicConfig(level=logging.WARN, force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Defining the host is optional and defaults to https://api.wordlift.io\n",
    "# See configuration.py for a list of all supported configuration parameters.\n",
    "api_url = 'https://api.wordlift.io'\n",
    "output_type = config.OUTPUT_TYPE or 'http://schema.org/WebPage'\n",
    "configuration = wordlift_client.Configuration(\n",
    "    host=api_url\n",
    ")\n",
    "\n",
    "# The client must configure the authentication and authorization parameters\n",
    "# in accordance with the API server security policy.\n",
    "# Examples for each auth method are provided below, use the example that\n",
    "# satisfies your auth use case.\n",
    "\n",
    "# Configure API key authorization: ApiKey\n",
    "configuration.api_key['ApiKey'] = config.WORDLIFT_KEY\n",
    "configuration.api_key_prefix['ApiKey'] = 'Key'\n",
    "\n",
    "\n",
    "async def kg():\n",
    "    from gql import gql\n",
    "\n",
    "    # Create a GraphQL client using the defined transport\n",
    "    client = GraphQLClientFactory(key=config.WORDLIFT_KEY).create()\n",
    "\n",
    "    # Define the GraphQL query\n",
    "    query = gql(\"\"\"\n",
    "        query getEntities($type_constraint: String!) {\n",
    "          entities(\n",
    "            query: { typeConstraint: { in: [$type_constraint] } }\n",
    "          ) {\n",
    "            id: iri\n",
    "            keywords: string(name: \"schema:keywords\")\n",
    "            url: string(name: \"schema:url\")\n",
    "          }\n",
    "        }\n",
    "    \"\"\")\n",
    "\n",
    "    variables = {\n",
    "        \"type_constraint\": output_type\n",
    "    }\n",
    "\n",
    "    # Asynchronous function to execute the query\n",
    "    async with client as session:\n",
    "        response = await session.execute(query, variable_values=variables)\n",
    "        return pd.DataFrame(response['entities'], columns=['id', 'keywords', 'url'])\n",
    "\n",
    "\n",
    "async def enrich(row: Series):\n",
    "    async def fetch(session: ClientSession, url: str) -> str:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "    async def parse(html: str) -> [str]:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Initialize an empty list to hold the combined results\n",
    "        combined_content = []\n",
    "\n",
    "        # Extract the 'mz:section' meta tag\n",
    "        section_meta = soup.find('meta', attrs={'property': 'mz:section'})\n",
    "        if section_meta and 'content' in section_meta.attrs:\n",
    "            section_content = [item.strip() for item in section_meta['content'].split(',')]\n",
    "            combined_content.extend(section_content)\n",
    "\n",
    "        # Extract the 'mz:subsection' meta tag\n",
    "        subsection_meta = soup.find('meta', attrs={'property': 'mz:subsection'})\n",
    "        if subsection_meta and 'content' in subsection_meta.attrs:\n",
    "            subsection_content = [item.strip() for item in subsection_meta['content'].split(',')]\n",
    "            combined_content.extend(subsection_content)\n",
    "\n",
    "        return combined_content\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        entity_url = row['url']\n",
    "        entity_id = row['id']\n",
    "        html = await fetch(session, entity_url)\n",
    "        keywords = await parse(html)\n",
    "\n",
    "        logger.info(f\"Processing entity: {entity_id} for URL: {entity_url}\")\n",
    "        resource = URIRef(entity_id)\n",
    "\n",
    "        payloads = []\n",
    "\n",
    "        for value in keywords:\n",
    "            payloads.append(\n",
    "                create_entity_patch_request(\n",
    "                    resource,\n",
    "                    URIRef('http://schema.org/keywords'),\n",
    "                    Literal(value)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # If the payloads are empty, exit.\n",
    "        if not payloads:\n",
    "            return\n",
    "\n",
    "        async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "            api_instance = wordlift_client.EntitiesApi(api_client)\n",
    "\n",
    "            logger.info(f\"Created {len(payloads)} patch requests for entity {entity_id}\")\n",
    "\n",
    "            try:\n",
    "                logger.info(f\"Sending patch request for entity {entity_id}\")\n",
    "                api_response = await api_instance.patch_entities(entity_id, payloads)\n",
    "                logger.info(f\"Successfully added attributes to entity {entity_id}\")\n",
    "                logger.debug(f\"API response: {api_response}\")\n",
    "            except ApiException as e:\n",
    "                logger.error(f\"ApiException when calling EntitiesApi->patch_entities: {e}\")\n",
    "                logger.error(f\"Response body: {e.body}\")\n",
    "                logger.error(f\"Response headers: {e.headers}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected exception when calling EntitiesApi->patch_entities: {e}\")\n",
    "\n",
    "            logger.info(f\"Finished processing entity: {entity_id}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    import asyncio\n",
    "    from os import cpu_count\n",
    "\n",
    "    def delayed(callback, concurrency=cpu_count() + 1):\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "        async def callback_with_semaphore(row):\n",
    "            async with sem:\n",
    "                return await callback(row)\n",
    "\n",
    "        return callback_with_semaphore\n",
    "\n",
    "    async def import_url(url_list: list[str]) -> None:\n",
    "        import wordlift_client\n",
    "\n",
    "        async with wordlift_client.ApiClient(configuration) as api_client:\n",
    "            imports_api = SitemapImportsApi(api_client)\n",
    "            request = SitemapImportRequest(\n",
    "                embedding=EmbeddingRequest(\n",
    "                    properties=[\"http://schema.org/headline\", \"http://schema.org/abstract\", \"http://schema.org/text\"]\n",
    "                ),\n",
    "                output_types=[output_type],\n",
    "                urls=url_list,\n",
    "                overwrite=True,\n",
    "                id_generator=\"headline-with-url-hash\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                await imports_api.create_sitemap_import(sitemap_import_request=request)\n",
    "            except Exception as e:\n",
    "                logger.error(\"Error importing URLs: %s\", e)\n",
    "\n",
    "    sitemap_df = adv.sitemap_to_df(config.SITEMAP_URL)\n",
    "    kg_df = await kg()\n",
    "    missing_url_list = list(set(sitemap_df['loc']) - set(kg_df['url']))\n",
    "    callback = delayed(import_url)\n",
    "    await tqdm.gather(*[callback([url]) for url in missing_url_list], total=len(missing_url_list))\n",
    "\n",
    "    no_keywords_df = kg_df[kg_df['keywords'].isna()]\n",
    "    await tqdm.gather(\n",
    "        *[delayed(enrich)(row) for index, row in no_keywords_df.iterrows()],\n",
    "        total=len(no_keywords_df)\n",
    "    )\n",
    "\n",
    "\n",
    "await main()\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziodave/Library/Caches/pypoetry/virtualenvs/onboarding-notebook-LVXebKIS-py3.12/lib/python3.12/site-packages/gql/transport/aiohttp.py:92: UserWarning: WARNING: By default, AIOHTTPTransport does not verify ssl certificates. This will be fixed in the next major version. You can set ssl=True to force the ssl certificate verification or ssl=False to disable this warning\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]\n",
      " 93%|█████████▎| 38/41 [00:43<00:00,  6.01it/s]"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
